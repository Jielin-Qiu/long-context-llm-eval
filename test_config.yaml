# AgentCodeEval Test Configuration File
# Optimized for testing and development with smaller numbers

api:
  # API Keys (set via environment variables for security)
  # openai_api_key: "your-openai-key-here"
  # anthropic_api_key: "your-anthropic-key-here" 
  # google_api_key: "your-google-key-here"
  # huggingface_token: "your-hf-token-here"
  
  # Rate limiting settings
  max_requests_per_minute: 60
  max_concurrent_requests: 10
  
  # Default models - üèÜ 3 Elite Models
  default_model_openai: "o3"                                  # ‚úÖ Elite: OpenAI o3 (reasoning model)
  default_model_anthropic: "claude-sonnet-4-20250514"         # ‚úÖ Elite: Claude Sonnet 4 via AWS Bedrock
  default_model_google: "gemini-2.5-pro"                      # ‚úÖ Elite: Gemini 2.5 Pro (latest)

data:
  # Local storage directories  
  output_dir: "./data/output"
  generated_dir: "./data/generated"
  
  # Synthetic generation settings
  supported_languages:
    - python      # #1: AI/ML dominance (23.88%, +8.72% growth)
    - cpp         # #2: High-performance, games, systems (11.37%, +0.84% growth)  
    - java        # #3: Enterprise, Android (10.66%, +1.79% growth)
    - c           # #4: Systems programming (9.84%, declining but essential)
    - csharp      # #5: Enterprise, Windows (.NET) (4.12%, Microsoft ecosystem)
    - javascript  # #6: Web standard, frontend/backend (3.78%, +0.61% growth)
    - typescript  # JS ecosystem: Enterprise web, type safety (75% of React projects)
    - go          # #8: Cloud-native, microservices (2.26%, +0.53% growth)
    - rust        # #13: Systems, security, performance (1.47%, +0.42% growth)
    - php         # #14: Web development, still widely used (1.14%, legacy but prevalent)
  
  # Project generation criteria (TESTING VALUES)
  min_files_per_project: 3
  max_files_per_project: 15
  projects_per_language: 1  # TESTING: 10 languages √ó 1 = 10 total projects
  
  # Complexity levels for synthetic projects
  complexity_distribution:
    easy: 0.50      # 50% easy projects (more for testing)
    medium: 0.30    # 30% medium projects
    hard: 0.15      # 15% hard projects
    expert: 0.05    # 5% expert projects
  
  # Generation quality controls
  min_complexity_score: 0.3
  max_complexity_score: 0.9
  min_documentation_ratio: 0.1

benchmark:
  # Scale parameters (TESTING VALUES)
  total_instances: 80  # 10 projects √ó 8 categories = 80 evaluation scenarios
  
  # Task category distribution (must sum to total_instances) 
  task_distribution:
    architectural_understanding: 10     # 10 languages √ó 1 project = 10 per category
    cross_file_refactoring: 10
    feature_implementation: 10
    bug_investigation: 10
    multi_session_development: 10
    code_comprehension: 10
    integration_testing: 10
    security_analysis: 10
  
  # Difficulty distribution (must sum to total_instances)
  difficulty_distribution:
    easy: 20      # 80 scenarios √∑ 4 difficulty levels = 20 each
    medium: 20
    hard: 20 
    expert: 20
  
  # Context length ranges (min_tokens, max_tokens)
  context_ranges:
    easy: [5000, 20000]      # Smaller for testing
    medium: [20000, 50000]   # Smaller for testing
    hard: [50000, 100000]    # Smaller for testing
    expert: [100000, 200000] # Smaller for testing
  
  # Information coverage requirements
  min_information_coverage: 0.7
  target_information_coverage:
    easy: 0.75
    medium: 0.80    # Lower for testing
    hard: 0.85      # Lower for testing
    expert: 0.90    # Lower for testing

evaluation:
  # Metric weights for CADS (Composite Agent Development Score)
  # Must sum to 1.0
  metric_weights:
    architectural_coherence: 0.20
    dependency_traversal: 0.20
    multi_session_memory: 0.20
    cross_file_reasoning: 0.15
    incremental_development: 0.15
    information_coverage: 0.10
  
  # Scoring thresholds
  score_thresholds:
    excellent:
      min: 4.0
      max: 5.0
    good:
      min: 3.0
      max: 4.0
    fair:
      min: 2.0
      max: 3.0
    poor:
      min: 0.0
      max: 2.0
  
  # Timeout settings (seconds) - PRODUCTION TIMEOUTS
  task_timeout: 180      # 3 minutes per task (shorter for testing)
  session_timeout: 900   # 15 minutes per session (shorter for testing) 