"""
Command Line Interface for AgentCodeEval
"""

import click
import os
import sys
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

from .core.config import Config

console = Console()


@click.group()
@click.version_option(version="0.1.0", prog_name="AgentCodeEval")
@click.pass_context  
def main(ctx):
    """AgentCodeEval: A Novel Benchmark for Evaluating Long-Context LLMs in Software Development Agent Tasks"""
    ctx.ensure_object(dict)


@main.command()
@click.option('--config-path', '-c', type=click.Path(), help='Path to configuration file')
@click.option('--save-config', '-s', type=click.Path(), help='Save configuration to file')
def setup(config_path, save_config):
    """Set up AgentCodeEval environment and configuration"""
    console.print(Panel.fit("ğŸš€ AgentCodeEval Setup", style="bold blue"))
    
    try:
        # Load configuration
        config = Config(config_path=config_path)
        
        # Validate configuration
        errors = config.validate()
        
        if errors:
            console.print("âŒ Configuration errors found:", style="bold red")
            for error in errors:
                console.print(f"  â€¢ {error}", style="red")
            
            # Check for available APIs (including AWS Bedrock)
            aws_configured = all([
                os.getenv('AWS_ACCESS_KEY_ID'),
                os.getenv('AWS_SECRET_ACCESS_KEY'), 
                os.getenv('AWS_SESSION_TOKEN')
            ])
            anthropic_available = config.api.anthropic_api_key or aws_configured
            
            if not any([config.api.openai_api_key, anthropic_available, config.api.google_api_key]):
                console.print("\nğŸ’¡ To fix API key issues, set environment variables:", style="yellow")
                console.print("  ğŸ† For our 3 Elite Models:")
                console.print("  export OPENAI_API_KEY='your-key-here'  # For o3")
                console.print("  export ANTHROPIC_API_KEY='your-key-here'  # For Claude Sonnet 4 (or use AWS)")
                console.print("  export GOOGLE_API_KEY='your-key-here'  # For Gemini 2.5 Pro")
                console.print("  # OR for Claude via AWS Bedrock:")
                console.print("  export AWS_ACCESS_KEY_ID='...' AWS_SECRET_ACCESS_KEY='...' AWS_SESSION_TOKEN='...'")
                
            sys.exit(1)
        
        # Display configuration summary
        console.print("âœ… Configuration validated successfully!", style="bold green")
        console.print(config.summary())
        
        # Save configuration if requested
        if save_config:
            config.save_to_file(save_config)
            console.print(f"ğŸ’¾ Configuration saved to: {save_config}", style="green")
            
        console.print("ğŸ¯ Setup complete! Ready to begin AgentCodeEval benchmark generation.", style="bold green")
        
    except Exception as e:
        console.print(f"âŒ Setup failed: {e}", style="bold red")
        sys.exit(1)


@main.command()
@click.option('--config-path', '-c', type=click.Path(), help='Path to configuration file')
def status(config_path):
    """Show current AgentCodeEval status and configuration"""
    try:
        config = Config(config_path=config_path)
        
        # Create status table
        table = Table(title="AgentCodeEval Status", style="cyan")
        table.add_column("Component", style="bold")
        table.add_column("Status", justify="center") 
        table.add_column("Details")
        
        # API status (checking our 3 Elite Models)
        # Check AWS credentials for Claude Bedrock access
        import os
        aws_configured = all([
            os.getenv('AWS_ACCESS_KEY_ID'),
            os.getenv('AWS_SECRET_ACCESS_KEY'), 
            os.getenv('AWS_SESSION_TOKEN')
        ])
        anthropic_available = config.api.anthropic_api_key or aws_configured
        
        apis = [
            ("OpenAI", config.api.openai_api_key),
            ("Anthropic", anthropic_available),  # Direct API or AWS Bedrock
            ("Google", config.api.google_api_key),
            # Removed HuggingFace - no longer needed with synthetic generation
        ]
        
        for name, key in apis:
            status_icon = "âœ…" if key else "âŒ"
            status_text = "Configured" if key else "Missing"
            table.add_row(f"{name} API", status_icon, status_text)
        
        # Directory status
        directories = [
            ("Cache Directory", config.data.cache_dir),
            ("Output Directory", config.data.output_dir), 
            ("Benchmark Directory", config.data.benchmark_dir)
        ]
        
        for name, path in directories:
            exists = Path(path).exists()
            status_icon = "âœ…" if exists else "âŒ"
            status_text = f"{'Exists' if exists else 'Missing'}: {path}"
            table.add_row(name, status_icon, status_text)
        
        # Benchmark configuration
        table.add_row("Benchmark Scale", "ğŸ“Š", f"{config.benchmark.total_instances:,} instances")
        table.add_row("Task Categories", "ğŸ“‹", f"{len(config.benchmark.task_distribution)} categories")
        table.add_row("Languages", "ğŸ”¤", f"{len(config.data.supported_languages)} languages")
        
        console.print(table)
        
        # Validation errors
        errors = config.validate()
        if errors:
            console.print("\nâš ï¸  Configuration Issues:", style="yellow")
            for error in errors:
                console.print(f"  â€¢ {error}", style="yellow")
        else:
            console.print("\nâœ… All systems ready!", style="bold green")
            
    except Exception as e:
        console.print(f"âŒ Status check failed: {e}", style="bold red")
        sys.exit(1)


@main.command()
@click.option('--config-path', '-c', type=click.Path(), help='Path to configuration file')
@click.option('--phase', type=click.Choice(['1', '2', '3', '4', 'all']), default='1', 
              help='Which implementation phase to run')
@click.option('--dry-run', is_flag=True, help='Show what would be done without executing')
def generate(config_path, phase, dry_run):
    """Generate AgentCodeEval benchmark instances"""
    console.print(Panel.fit(f"ğŸ—ï¸  AgentCodeEval Generation - Phase {phase}", style="bold green"))
    
    if dry_run:
        console.print("ğŸ” DRY RUN MODE - No actual generation will occur", style="yellow")
    
    try:
        config = Config(config_path=config_path)
        
        # Validate configuration
        errors = config.validate()
        if errors:
            console.print("âŒ Configuration errors found:", style="bold red")
            for error in errors:
                console.print(f"  â€¢ {error}", style="red")
            sys.exit(1)
        
        if phase == '1' or phase == 'all':
            console.print("ğŸ¯ Phase 1: Synthetic Project Generation", style="bold")
            if not dry_run:
                import asyncio
                from .generation.synthetic_generator import SyntheticProjectGenerator, ProjectDomain, ProjectComplexity
                asyncio.run(run_phase_1_generation(config))
            else:
                console.print("  â€¢ Generate synthetic multi-file projects")
                console.print("  â€¢ 10 domains Ã— 4 complexity levels Ã— 6 languages")
                console.print("  â€¢ Production-quality code with tests & docs")
                console.print(f"  â€¢ Target: 1,200 synthetic projects")
                
        if phase == '2' or phase == 'all':
            console.print("ğŸ® Phase 2: Agent Evaluation Scenario Creation", style="bold")
            if not dry_run:
                console.print("â³ [yellow]Coming soon: Convert projects to agent evaluation tasks[/yellow]")
                # TODO: Implement scenario generation from synthetic projects
            else:
                console.print("  â€¢ Convert synthetic projects to evaluation scenarios")
                console.print("  â€¢ Create bug investigation tasks")
                console.print("  â€¢ Generate feature implementation challenges") 
                console.print(f"  â€¢ Target: 8 task categories Ã— 1,200 projects")
                
        if phase == '3' or phase == 'all':
            console.print("ğŸ“ˆ Phase 3: 12,000 Instance Generation", style="bold") 
            if not dry_run:
                console.print("â³ [yellow]Coming soon: Generate 12,000 evaluation instances[/yellow]")
                # TODO: Generate 10 evaluation instances per project
            else:
                console.print("  â€¢ Generate 10 instances per synthetic project")
                console.print("  â€¢ Apply progressive complexity scaling")
                console.print("  â€¢ Multi-session development scenarios")
                console.print(f"  â€¢ Target: {config.benchmark.total_instances:,} instances")
                
        if phase == '4' or phase == 'all':
            console.print("ğŸ¯ Phase 4: Reference Solution & Validation", style="bold")
            if not dry_run:
                console.print("â³ [yellow]Coming soon: Generate ground truth solutions[/yellow]")
                # TODO: Generate reference solutions using multi-LLM approach
            else:
                console.print("  â€¢ Multi-LLM reference solution generation")
                console.print("  â€¢ Automated validation and quality checks")
                console.print("  â€¢ Novel agent-specific evaluation metrics")
                console.print(f"  â€¢ Target: Ground truth for all 12,000 instances")
        
        console.print("ğŸ‰ Generation phase complete!", style="bold green")
        
    except Exception as e:
        console.print(f"âŒ Generation failed: {e}", style="bold red")
        sys.exit(1)


@main.command()
@click.option('--config-path', '-c', type=click.Path(), help='Path to configuration file')
@click.option('--model', '-m', multiple=True, help='Model to evaluate (can specify multiple)')
@click.option('--task-category', '-t', multiple=True, help='Task category to evaluate')
@click.option('--difficulty', '-d', type=click.Choice(['easy', 'medium', 'hard', 'expert']),
              help='Difficulty level to evaluate')
@click.option('--output-file', '-o', type=click.Path(), help='Output file for results')
def evaluate(config_path, model, task_category, difficulty, output_file):
    """Evaluate models on AgentCodeEval benchmark"""
    console.print(Panel.fit("ğŸ§ª AgentCodeEval Evaluation", style="bold purple"))
    
    try:
        config = Config(config_path=config_path)
        
        # Show evaluation parameters
        console.print("ğŸ“‹ Evaluation Parameters:", style="bold")
        console.print(f"  â€¢ Models: {list(model) if model else 'All available'}")
        console.print(f"  â€¢ Categories: {list(task_category) if task_category else 'All categories'}")
        console.print(f"  â€¢ Difficulty: {difficulty if difficulty else 'All levels'}")
        console.print(f"  â€¢ Output: {output_file if output_file else 'Standard output'}")
        
        from .evaluation.evaluator import run_evaluation
        results = run_evaluation(config, model, task_category, difficulty)
        
        # Display results
        if results:
            console.print("ğŸ“Š Evaluation Results:", style="bold green")
            # TODO: Format and display results
            console.print(results)
            
            if output_file:
                # TODO: Save results to file
                console.print(f"ğŸ’¾ Results saved to: {output_file}", style="green")
        
    except Exception as e:
        console.print(f"âŒ Evaluation failed: {e}", style="bold red")
        sys.exit(1)


@main.command()
def version():
    """Show AgentCodeEval version information"""
    console.print("ğŸ”§ AgentCodeEval v0.1.0", style="bold blue")
    console.print("A Novel Benchmark for Evaluating Long-Context Language Models")
    console.print("in Software Development Agent Tasks")
    console.print("\nFor more information: https://github.com/AgentCodeEval/AgentCodeEval")


async def run_phase_1_generation(config):
    """Run Phase 1: Synthetic Project Generation"""
    from .generation.synthetic_generator import SyntheticProjectGenerator, ProjectDomain, ProjectComplexity
    
    console.print("\nğŸ¯ [bold]Synthetic Project Generation Pipeline[/bold]")
    console.print("=" * 60)
    
    generator = SyntheticProjectGenerator(config)
    
    # Target: projects per language from config
    languages = config.data.supported_languages
    projects_per_language = config.data.projects_per_language
    
    console.print(f"ğŸ“Š Target: {len(languages)} languages Ã— {projects_per_language} projects = {len(languages) * projects_per_language} total")
    console.print(f"ğŸŒ Languages: {', '.join(languages)}")
    
    domains = list(ProjectDomain)[:6]  # Use first 6 domains for balanced distribution
    complexities = list(ProjectComplexity)
    
    total_generated = 0
    
    for language in languages:
        console.print(f"\nğŸ”¤ [bold cyan]Processing {language}...[/bold cyan]")
        
        lang_projects = 0
        projects_per_domain = projects_per_language // len(domains)
        
        for domain in domains:
            for complexity in complexities:
                if lang_projects >= projects_per_language:
                    break
                
                try:
                    with console.status(f"[bold green]Generating {complexity.value} {domain.value} in {language}..."):
                        
                        project = await generator.generate_complete_project(
                            domain=domain,
                            complexity=complexity,
                            language=language
                        )
                        
                        project_path = await generator.save_project(project)
                        
                        console.print(f"   âœ… {project.specification.name} ({len(project.files)} files, {sum(len(f.content) for f in project.files):,} chars)")
                        
                        lang_projects += 1
                        total_generated += 1
                        
                except Exception as e:
                    console.print(f"   âŒ Failed {complexity.value} {domain.value}: {str(e)[:50]}...")
                    continue
            
            if lang_projects >= projects_per_language:
                break
        
        console.print(f"   ğŸ“ˆ {language}: {lang_projects} projects generated")
    
    console.print(f"\nğŸ‰ [bold green]Phase 1 Complete![/bold green]")
    console.print(f"   ğŸ“Š Generated: {total_generated} synthetic projects")
    console.print(f"   ğŸ“ Location: {config.data.generated_dir}")
    console.print(f"   ğŸ’¾ Total size: ~{total_generated * 50000:,} characters of code")


if __name__ == '__main__':
    main() 